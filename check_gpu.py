# check_gpu.py
import torch
import sys
import platform

print("="*50)
print("ğŸ–¥ï¸  ì‹œìŠ¤í…œ ì •ë³´")
print("="*50)
print(f"Python ë²„ì „: {sys.version}")
print(f"ìš´ì˜ì²´ì œ: {platform.system()} {platform.release()}")
print()

print("="*50)
print("ğŸ® GPU ì •ë³´")
print("="*50)

# CUDA ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€
cuda_available = torch.cuda.is_available()
print(f"CUDA ì‚¬ìš© ê°€ëŠ¥: {cuda_available}")

if cuda_available:
    # CUDA ë²„ì „
    print(f"CUDA ë²„ì „: {torch.version.cuda}")
    
    # GPU ê°œìˆ˜
    gpu_count = torch.cuda.device_count()
    print(f"ì‚¬ìš© ê°€ëŠ¥í•œ GPU ê°œìˆ˜: {gpu_count}")
    
    # ê° GPU ì •ë³´
    for i in range(gpu_count):
        print(f"\nGPU {i}: {torch.cuda.get_device_name(i)}")
        print(f"  - ë©”ëª¨ë¦¬: {torch.cuda.get_device_properties(i).total_memory / 1024**3:.2f} GB")
        print(f"  - í˜„ì¬ ì‚¬ìš©ì¤‘: {torch.cuda.memory_allocated(i) / 1024**3:.2f} GB")
        print(f"  - ìºì‹œ: {torch.cuda.memory_reserved(i) / 1024**3:.2f} GB")
    
    # í˜„ì¬ GPU
    print(f"\ní˜„ì¬ ê¸°ë³¸ GPU: {torch.cuda.current_device()}")
else:
    print("\nâš ï¸ GPUë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤!")
    print("ë‹¤ìŒì„ í™•ì¸í•´ì£¼ì„¸ìš”:")
    print("1. NVIDIA GPUê°€ ì„¤ì¹˜ë˜ì–´ ìˆëŠ”ì§€")
    print("2. CUDAê°€ ì„¤ì¹˜ë˜ì–´ ìˆëŠ”ì§€")
    print("3. PyTorch GPU ë²„ì „ì´ ì„¤ì¹˜ë˜ì–´ ìˆëŠ”ì§€")

# PyTorch ì„¤ì¹˜ í™•ì¸
print("\n" + "="*50)
print("ğŸ“¦ PyTorch ì„¤ì¹˜ ì •ë³´")
print("="*50)
print(f"PyTorch ë²„ì „: {torch.__version__}")
print(f"cuDNN í™œì„±í™”: {torch.backends.cudnn.enabled if cuda_available else 'N/A'}")